# Transactional Workloads & Analytical Workloads

## Transactional Data Processing
- A transactional system records transactions that encapsulate specific events the organisation wants to track.
- Examples could be:
    - the movement of money between accounts in a banking system
    - a record of an item bought from a website
- A transaction is a small, discrete unit of work
- Transactional systems are often referred to as Online Transactional Processing (OLTP)

### OLTP
- OLTP solutions rely on a DB system in which the data storage is optimised for CRUD (Create, Read, Update, Delete) operations
- Typically used to support live applications that process business data - often referred to as _line of business_
- OLTP systems enforce transactions that support ACID semantics which are:
    - Atomicity
        - Each statement in a transaction is treated as a single unit
        - Either the entire statement is executed or none of it is executed
        - Prevents data loss and corruption if something fails 
    - Consistency
        - Ensures transactions only make changes to tables in predefined and predictable ways
        - Ensures that corruption or errors in data do not create unintended consequences to the integrity of your table
    - Isolation
        - Concurrent transactions don't interfere with one another
        - Each request can occur as though they were occuring one by one, even if they're actually occuring simultaneously
        - Ensures consistent database state
    - Durability
        - Ensures changes to your data made by succesfully executed transactions will be saved, even in the event of system failure


## Analytical Data Processing
- Typically uses read-only (or read-mostly) systems that store vast volumes of historical data or business metrics
- Specific details for an analytical processing system can vary between solutions, but a common architecture for enterprise-scale analytics is outlined below:
    1. Operational data is extracted, transformed and loaded (ETL) into a data lake for analysis
    2. Data is loaded into a schema of tables. This could be a Spark-based data lakehouse or a data warehouse with a fully relational SQL engine
    3. Data in the data warehouse may be aggregated and loaded into an online analytical processing (OLAP) model or _cube_. Star or Snowflake schemas can be applied to seperate data into fact and dimension tables
    4. Data in the data lake, data warehouse and analytical model can be queried to produce reports, visualisations & dashboards

#### Data Lakes
- A container/storage solution that is made for large amounts of all data types (structured, unstructured, semi-structured).
- Often used by Data Scientists for ML and AI 
- Data contained can go through the ETL process to enter a data warehouse with the applied schema

#### Data Warehouse
- Adheres to a rigid schema in a relational model
- Data is refreshed from multiple source systems and stores both historical and current data
- Optimised for read operations for analysis, reporting and data visualisation
- Often used by data analysts

#### Data Lakehouse
- Combines the flexible and scalable storage of a data lake with the relational querying semantics of a data warehouse into one data management solution
- Can meet the needs of both business intelligence and data science workstreams
